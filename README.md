 <h6> VLMFromScratch </h6>
 
<h5> 
  
Built a Multimodal Vision Language Model in PyTorch, coding the Contrastive Learning, Vision Transformer, and multi-head attention mechanisms from scratch.

Developed the SigLip architecture, implementing the encoder, feed-forward network (FFN), and image feature projections, and incorporated batch normalization, layer normalization, and rotary positional embeddings.

Created the PaliGemma architecture, integrating weight tying, KV-Cache, and Top-P sampling to enhance model performance and inference efficiency.

Implemented multimodal processing features including grouped query attention, RMS normalization, and image features projection to improve the modelâ€™s handling of visual and textual data.

</h5>
